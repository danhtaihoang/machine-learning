{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Neural Network\n",
    "# very effective for natural language processing, time series, and speech recognition \n",
    "\n",
    "# Long Short-Term Memory network (LSTM RNN):\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv'\n",
    "df = pd.read_csv(path, header=0, index_col=0, squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Month\n",
       "1960-01     6550\n",
       "1960-02     8728\n",
       "1960-03    12026\n",
       "1960-04    14395\n",
       "1960-05    14587\n",
       "Name: Sales, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the values\n",
    "values = df.values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6550.,  8728., 12026., 14395., 14587., 13791.,  9498.,  8251.,\n",
       "        7049.,  9545.,  9364.,  8456.,  7237.,  9374., 11837., 13784.,\n",
       "       15926., 13821., 11143.,  7975.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequence)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps\n",
    "\t\t# check if we are beyond the sequence\n",
    "\t\tif end_ix > len(sequence)-1:\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the window size\n",
    "n_steps = 5\n",
    "# split into samples, take values of the last 5 months to predict the current value.\n",
    "X, y = split_sequence(values, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6550.,  8728., 12026., 14395., 14587.],\n",
       "       [ 8728., 12026., 14395., 14587., 13791.],\n",
       "       [12026., 14395., 14587., 13791.,  9498.],\n",
       "       [14395., 14587., 13791.,  9498.,  8251.],\n",
       "       [14587., 13791.,  9498.,  8251.,  7049.],\n",
       "       [13791.,  9498.,  8251.,  7049.,  9545.],\n",
       "       [ 9498.,  8251.,  7049.,  9545.,  9364.],\n",
       "       [ 8251.,  7049.,  9545.,  9364.,  8456.],\n",
       "       [ 7049.,  9545.,  9364.,  8456.,  7237.],\n",
       "       [ 9545.,  9364.,  8456.,  7237.,  9374.]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13791.,  9498.,  8251.,  7049.,  9545.,  9364.,  8456.,  7237.,\n",
       "        9374., 11837.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape into [samples, timesteps, features]\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 5, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91, 5, 1) (12, 5, 1) (91,) (12,)\n"
     ]
    }
   ],
   "source": [
    "# split into train/test\n",
    "n_test = 12\n",
    "X_train, X_test, y_train, y_test = X[:-n_test], X[-n_test:], y[:-n_test], y[-n_test:]\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', kernel_initializer='he_normal', input_shape=(n_steps,1)))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(50, activation='relu', kernel_initializer='he_normal'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91 samples, validate on 12 samples\n",
      "Epoch 1/350\n",
      "91/91 - 1s - loss: 784243296.0000 - mae: 25297.9062 - val_loss: 303358720.0000 - val_mae: 15483.7158\n",
      "Epoch 2/350\n",
      "91/91 - 0s - loss: 142243041.9341 - mae: 10220.2412 - val_loss: 125287320.0000 - val_mae: 8041.5488\n",
      "Epoch 3/350\n",
      "91/91 - 0s - loss: 112326809.8462 - mae: 8250.2422 - val_loss: 246690896.0000 - val_mae: 12664.3408\n",
      "Epoch 4/350\n",
      "91/91 - 0s - loss: 160094989.0110 - mae: 9953.9951 - val_loss: 208929616.0000 - val_mae: 12313.3750\n",
      "Epoch 5/350\n",
      "91/91 - 0s - loss: 107044406.3297 - mae: 8092.7734 - val_loss: 67803864.0000 - val_mae: 6737.9141\n",
      "Epoch 6/350\n",
      "91/91 - 0s - loss: 45736547.3407 - mae: 5195.0664 - val_loss: 24253446.0000 - val_mae: 4294.8428\n",
      "Epoch 7/350\n",
      "91/91 - 0s - loss: 34270359.4286 - mae: 4720.5093 - val_loss: 59373968.0000 - val_mae: 6296.0562\n",
      "Epoch 8/350\n",
      "91/91 - 0s - loss: 48313893.9780 - mae: 5518.5688 - val_loss: 58491596.0000 - val_mae: 5864.2368\n",
      "Epoch 9/350\n",
      "91/91 - 0s - loss: 40664864.7033 - mae: 5032.4956 - val_loss: 30024026.0000 - val_mae: 4143.1699\n",
      "Epoch 10/350\n",
      "91/91 - 0s - loss: 26529814.7253 - mae: 4051.4561 - val_loss: 20248322.0000 - val_mae: 3750.7356\n",
      "Epoch 11/350\n",
      "91/91 - 0s - loss: 20334580.3297 - mae: 3504.5635 - val_loss: 25963478.0000 - val_mae: 4343.3706\n",
      "Epoch 12/350\n",
      "91/91 - 0s - loss: 22915209.0989 - mae: 3708.4824 - val_loss: 22726214.0000 - val_mae: 4267.9771\n",
      "Epoch 13/350\n",
      "91/91 - 0s - loss: 18158735.4066 - mae: 3411.4382 - val_loss: 13625349.0000 - val_mae: 3447.2139\n",
      "Epoch 14/350\n",
      "91/91 - 0s - loss: 14786945.5604 - mae: 3110.4180 - val_loss: 15152429.0000 - val_mae: 3105.7168\n",
      "Epoch 15/350\n",
      "91/91 - 0s - loss: 15882429.4286 - mae: 3161.4697 - val_loss: 18182160.0000 - val_mae: 3294.9238\n",
      "Epoch 16/350\n",
      "91/91 - 0s - loss: 16502905.1648 - mae: 3199.9800 - val_loss: 17454702.0000 - val_mae: 3502.1318\n",
      "Epoch 17/350\n",
      "91/91 - 0s - loss: 15805000.1758 - mae: 3196.6902 - val_loss: 21484774.0000 - val_mae: 4000.0833\n",
      "Epoch 18/350\n",
      "91/91 - 0s - loss: 13975531.8681 - mae: 3048.6538 - val_loss: 22176388.0000 - val_mae: 4090.7659\n",
      "Epoch 19/350\n",
      "91/91 - 0s - loss: 13311308.6923 - mae: 2983.2256 - val_loss: 20475630.0000 - val_mae: 3891.1213\n",
      "Epoch 20/350\n",
      "91/91 - 0s - loss: 12650435.0769 - mae: 2917.8740 - val_loss: 20263480.0000 - val_mae: 3785.2490\n",
      "Epoch 21/350\n",
      "91/91 - 0s - loss: 12185538.0440 - mae: 2864.3362 - val_loss: 19861562.0000 - val_mae: 3625.9910\n",
      "Epoch 22/350\n",
      "91/91 - 0s - loss: 11331330.5275 - mae: 2731.6631 - val_loss: 19950608.0000 - val_mae: 3708.8242\n",
      "Epoch 23/350\n",
      "91/91 - 0s - loss: 11060478.5165 - mae: 2686.1958 - val_loss: 19891774.0000 - val_mae: 3767.3674\n",
      "Epoch 24/350\n",
      "91/91 - 0s - loss: 11113964.7802 - mae: 2674.3474 - val_loss: 19658394.0000 - val_mae: 3782.9885\n",
      "Epoch 25/350\n",
      "91/91 - 0s - loss: 11047433.8132 - mae: 2658.7114 - val_loss: 19158546.0000 - val_mae: 3747.7019\n",
      "Epoch 26/350\n",
      "91/91 - 0s - loss: 10762982.4066 - mae: 2611.0750 - val_loss: 20026196.0000 - val_mae: 3913.7188\n",
      "Epoch 27/350\n",
      "91/91 - 0s - loss: 10164149.0110 - mae: 2607.9609 - val_loss: 23931560.0000 - val_mae: 4600.4360\n",
      "Epoch 28/350\n",
      "91/91 - 0s - loss: 11463569.1429 - mae: 2735.1531 - val_loss: 20737512.0000 - val_mae: 4163.0762\n",
      "Epoch 29/350\n",
      "91/91 - 0s - loss: 11498301.2637 - mae: 2777.3501 - val_loss: 19881784.0000 - val_mae: 3815.0164\n",
      "Epoch 30/350\n",
      "91/91 - 0s - loss: 11376447.3297 - mae: 2706.4045 - val_loss: 19650686.0000 - val_mae: 3740.9402\n",
      "Epoch 31/350\n",
      "91/91 - 0s - loss: 11182361.5495 - mae: 2691.0332 - val_loss: 19372792.0000 - val_mae: 3772.5637\n",
      "Epoch 32/350\n",
      "91/91 - 0s - loss: 10917875.5055 - mae: 2671.2300 - val_loss: 19565898.0000 - val_mae: 3950.4417\n",
      "Epoch 33/350\n",
      "91/91 - 0s - loss: 11026735.7143 - mae: 2728.6165 - val_loss: 19563418.0000 - val_mae: 3935.3469\n",
      "Epoch 34/350\n",
      "91/91 - 0s - loss: 11042112.5055 - mae: 2706.2170 - val_loss: 19119946.0000 - val_mae: 3653.6848\n",
      "Epoch 35/350\n",
      "91/91 - 0s - loss: 10815130.3077 - mae: 2653.9922 - val_loss: 19019906.0000 - val_mae: 3703.6775\n",
      "Epoch 36/350\n",
      "91/91 - 0s - loss: 10464090.6813 - mae: 2636.5110 - val_loss: 18883694.0000 - val_mae: 3674.1956\n",
      "Epoch 37/350\n",
      "91/91 - 0s - loss: 10627881.4725 - mae: 2668.1011 - val_loss: 18707858.0000 - val_mae: 3747.0398\n",
      "Epoch 38/350\n",
      "91/91 - 0s - loss: 10266781.6044 - mae: 2621.1797 - val_loss: 18548878.0000 - val_mae: 3605.4043\n",
      "Epoch 39/350\n",
      "91/91 - 0s - loss: 10308124.8571 - mae: 2592.7207 - val_loss: 18473054.0000 - val_mae: 3554.5215\n",
      "Epoch 40/350\n",
      "91/91 - 0s - loss: 10143579.4835 - mae: 2554.7537 - val_loss: 18197060.0000 - val_mae: 3599.7668\n",
      "Epoch 41/350\n",
      "91/91 - 0s - loss: 9977024.0549 - mae: 2576.8018 - val_loss: 18160406.0000 - val_mae: 3706.3284\n",
      "Epoch 42/350\n",
      "91/91 - 0s - loss: 10037437.9121 - mae: 2596.7300 - val_loss: 17949154.0000 - val_mae: 3680.6965\n",
      "Epoch 43/350\n",
      "91/91 - 0s - loss: 9878106.6813 - mae: 2548.2136 - val_loss: 17666910.0000 - val_mae: 3517.1086\n",
      "Epoch 44/350\n",
      "91/91 - 0s - loss: 9851380.7582 - mae: 2509.1982 - val_loss: 17612338.0000 - val_mae: 3431.2800\n",
      "Epoch 45/350\n",
      "91/91 - 0s - loss: 9880461.0165 - mae: 2517.4019 - val_loss: 17369700.0000 - val_mae: 3562.5469\n",
      "Epoch 46/350\n",
      "91/91 - 0s - loss: 9901931.3242 - mae: 2560.6396 - val_loss: 17383382.0000 - val_mae: 3608.6472\n",
      "Epoch 47/350\n",
      "91/91 - 0s - loss: 9567839.3187 - mae: 2507.8943 - val_loss: 17330282.0000 - val_mae: 3425.4563\n",
      "Epoch 48/350\n",
      "91/91 - 0s - loss: 9836838.1868 - mae: 2501.5117 - val_loss: 17362252.0000 - val_mae: 3383.7917\n",
      "Epoch 49/350\n",
      "91/91 - 0s - loss: 9618404.1648 - mae: 2500.7373 - val_loss: 17028270.0000 - val_mae: 3580.5300\n",
      "Epoch 50/350\n",
      "91/91 - 0s - loss: 9552062.3956 - mae: 2539.6287 - val_loss: 17001932.0000 - val_mae: 3603.0496\n",
      "Epoch 51/350\n",
      "91/91 - 0s - loss: 9373168.2088 - mae: 2502.8665 - val_loss: 16729137.0000 - val_mae: 3404.8455\n",
      "Epoch 52/350\n",
      "91/91 - 0s - loss: 9274520.0330 - mae: 2419.6733 - val_loss: 17025942.0000 - val_mae: 3318.7715\n",
      "Epoch 53/350\n",
      "91/91 - 0s - loss: 9398974.6264 - mae: 2432.8623 - val_loss: 16615555.0000 - val_mae: 3359.0496\n",
      "Epoch 54/350\n",
      "91/91 - 0s - loss: 9512739.1264 - mae: 2494.1636 - val_loss: 16628101.0000 - val_mae: 3571.3555\n",
      "Epoch 55/350\n",
      "91/91 - 0s - loss: 9286874.8132 - mae: 2482.5293 - val_loss: 16334048.0000 - val_mae: 3418.1143\n",
      "Epoch 56/350\n",
      "91/91 - 0s - loss: 9647123.8242 - mae: 2493.1541 - val_loss: 16758971.0000 - val_mae: 3325.6882\n",
      "Epoch 57/350\n",
      "91/91 - 0s - loss: 9246312.4286 - mae: 2474.1531 - val_loss: 15932967.0000 - val_mae: 3346.8074\n",
      "Epoch 58/350\n",
      "91/91 - 0s - loss: 9503399.1868 - mae: 2549.8457 - val_loss: 15854697.0000 - val_mae: 3552.7625\n",
      "Epoch 59/350\n",
      "91/91 - 0s - loss: 9495227.8901 - mae: 2576.4631 - val_loss: 15481816.0000 - val_mae: 3434.7024\n",
      "Epoch 60/350\n",
      "91/91 - 0s - loss: 9137378.4121 - mae: 2476.4417 - val_loss: 15685109.0000 - val_mae: 3267.5056\n",
      "Epoch 61/350\n",
      "91/91 - 0s - loss: 9634927.2418 - mae: 2503.3359 - val_loss: 15827575.0000 - val_mae: 3187.1797\n",
      "Epoch 62/350\n",
      "91/91 - 0s - loss: 9689938.7802 - mae: 2551.0664 - val_loss: 15696725.0000 - val_mae: 3453.7859\n",
      "Epoch 63/350\n",
      "91/91 - 0s - loss: 9249714.5604 - mae: 2549.5090 - val_loss: 16026293.0000 - val_mae: 3467.8718\n",
      "Epoch 64/350\n",
      "91/91 - 0s - loss: 9035288.2747 - mae: 2527.6128 - val_loss: 15552763.0000 - val_mae: 3357.4021\n",
      "Epoch 65/350\n",
      "91/91 - 0s - loss: 9219338.0220 - mae: 2492.9419 - val_loss: 15651507.0000 - val_mae: 3229.5615\n",
      "Epoch 66/350\n",
      "91/91 - 0s - loss: 8771581.4121 - mae: 2407.9692 - val_loss: 15497693.0000 - val_mae: 3382.3887\n",
      "Epoch 67/350\n",
      "91/91 - 0s - loss: 8699060.8626 - mae: 2435.5940 - val_loss: 15325144.0000 - val_mae: 3350.4424\n",
      "Epoch 68/350\n",
      "91/91 - 0s - loss: 8704313.6099 - mae: 2428.5479 - val_loss: 15315325.0000 - val_mae: 3321.4697\n",
      "Epoch 69/350\n",
      "91/91 - 0s - loss: 8680491.3791 - mae: 2416.3608 - val_loss: 15206985.0000 - val_mae: 3299.0461\n",
      "Epoch 70/350\n",
      "91/91 - 0s - loss: 8472849.2198 - mae: 2353.7822 - val_loss: 16008437.0000 - val_mae: 3446.2278\n",
      "Epoch 71/350\n",
      "91/91 - 0s - loss: 8426591.9121 - mae: 2359.3928 - val_loss: 15646685.0000 - val_mae: 3299.0813\n",
      "Epoch 72/350\n",
      "91/91 - 0s - loss: 8365836.1978 - mae: 2330.7871 - val_loss: 15694808.0000 - val_mae: 3301.5247\n",
      "Epoch 73/350\n",
      "91/91 - 0s - loss: 8477758.7363 - mae: 2362.4443 - val_loss: 15335664.0000 - val_mae: 3286.3215\n",
      "Epoch 74/350\n",
      "91/91 - 0s - loss: 8324599.1648 - mae: 2379.7380 - val_loss: 15861748.0000 - val_mae: 3481.4524\n",
      "Epoch 75/350\n",
      "91/91 - 0s - loss: 8515121.6703 - mae: 2374.6394 - val_loss: 15497860.0000 - val_mae: 3384.5107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/350\n",
      "91/91 - 0s - loss: 8151333.3022 - mae: 2254.6257 - val_loss: 15288117.0000 - val_mae: 3321.4490\n",
      "Epoch 77/350\n",
      "91/91 - 0s - loss: 8046272.0440 - mae: 2220.0791 - val_loss: 15956264.0000 - val_mae: 3352.0066\n",
      "Epoch 78/350\n",
      "91/91 - 0s - loss: 7996188.1813 - mae: 2223.6743 - val_loss: 15938136.0000 - val_mae: 3286.0701\n",
      "Epoch 79/350\n",
      "91/91 - 0s - loss: 7800028.4725 - mae: 2202.7473 - val_loss: 15496085.0000 - val_mae: 3280.8711\n",
      "Epoch 80/350\n",
      "91/91 - 0s - loss: 7959014.2033 - mae: 2172.1855 - val_loss: 15494360.0000 - val_mae: 3292.1287\n",
      "Epoch 81/350\n",
      "91/91 - 0s - loss: 7833818.9341 - mae: 2149.8760 - val_loss: 15316493.0000 - val_mae: 3261.9824\n",
      "Epoch 82/350\n",
      "91/91 - 0s - loss: 7723730.2967 - mae: 2196.9487 - val_loss: 15314712.0000 - val_mae: 3311.4597\n",
      "Epoch 83/350\n",
      "91/91 - 0s - loss: 7700019.6978 - mae: 2204.2400 - val_loss: 14634997.0000 - val_mae: 3213.4597\n",
      "Epoch 84/350\n",
      "91/91 - 0s - loss: 7926125.5385 - mae: 2233.2861 - val_loss: 14298620.0000 - val_mae: 3188.8782\n",
      "Epoch 85/350\n",
      "91/91 - 0s - loss: 8155969.0220 - mae: 2284.8574 - val_loss: 10240073.0000 - val_mae: 2719.9255\n",
      "Epoch 86/350\n",
      "91/91 - 0s - loss: 8088249.9011 - mae: 2350.4744 - val_loss: 9929413.0000 - val_mae: 2621.3328\n",
      "Epoch 87/350\n",
      "91/91 - 0s - loss: 8143028.5165 - mae: 2357.7239 - val_loss: 10258228.0000 - val_mae: 2719.4221\n",
      "Epoch 88/350\n",
      "91/91 - 0s - loss: 9776929.3297 - mae: 2572.7556 - val_loss: 12688011.0000 - val_mae: 2953.9558\n",
      "Epoch 89/350\n",
      "91/91 - 0s - loss: 11749728.3297 - mae: 2766.5388 - val_loss: 16599527.0000 - val_mae: 3509.9846\n",
      "Epoch 90/350\n",
      "91/91 - 0s - loss: 11606199.7143 - mae: 2708.8677 - val_loss: 19084382.0000 - val_mae: 3773.5703\n",
      "Epoch 91/350\n",
      "91/91 - 0s - loss: 11769249.2747 - mae: 2753.7305 - val_loss: 18152160.0000 - val_mae: 3539.4932\n",
      "Epoch 92/350\n",
      "91/91 - 0s - loss: 13869682.7143 - mae: 3006.4204 - val_loss: 20086310.0000 - val_mae: 3310.6414\n",
      "Epoch 93/350\n",
      "91/91 - 0s - loss: 12022145.4835 - mae: 2725.3469 - val_loss: 17740926.0000 - val_mae: 3598.5129\n",
      "Epoch 94/350\n",
      "91/91 - 0s - loss: 11390057.1758 - mae: 2764.8789 - val_loss: 19400134.0000 - val_mae: 4048.2336\n",
      "Epoch 95/350\n",
      "91/91 - 0s - loss: 10306216.0824 - mae: 2540.9863 - val_loss: 17630910.0000 - val_mae: 3568.1360\n",
      "Epoch 96/350\n",
      "91/91 - 0s - loss: 9980178.5604 - mae: 2501.0994 - val_loss: 17446872.0000 - val_mae: 3121.2410\n",
      "Epoch 97/350\n",
      "91/91 - 0s - loss: 10225996.0220 - mae: 2540.4172 - val_loss: 18034824.0000 - val_mae: 3566.5146\n",
      "Epoch 98/350\n",
      "91/91 - 0s - loss: 9765265.7308 - mae: 2491.7327 - val_loss: 20110938.0000 - val_mae: 4118.8862\n",
      "Epoch 99/350\n",
      "91/91 - 0s - loss: 10336914.2637 - mae: 2577.4939 - val_loss: 16674800.0000 - val_mae: 3683.0618\n",
      "Epoch 100/350\n",
      "91/91 - 0s - loss: 8950722.6923 - mae: 2314.4224 - val_loss: 15553831.0000 - val_mae: 3112.6384\n",
      "Epoch 101/350\n",
      "91/91 - 0s - loss: 9694348.6264 - mae: 2456.1350 - val_loss: 14148899.0000 - val_mae: 3086.7786\n",
      "Epoch 102/350\n",
      "91/91 - 0s - loss: 8908772.5824 - mae: 2334.1013 - val_loss: 15736651.0000 - val_mae: 3493.6184\n",
      "Epoch 103/350\n",
      "91/91 - 0s - loss: 8743899.5714 - mae: 2373.0720 - val_loss: 14794553.0000 - val_mae: 3279.7725\n",
      "Epoch 104/350\n",
      "91/91 - 0s - loss: 8483543.8022 - mae: 2276.3225 - val_loss: 13546243.0000 - val_mae: 3051.6194\n",
      "Epoch 105/350\n",
      "91/91 - 0s - loss: 8536671.4286 - mae: 2282.1504 - val_loss: 13799584.0000 - val_mae: 3096.5183\n",
      "Epoch 106/350\n",
      "91/91 - 0s - loss: 8345639.9396 - mae: 2257.0830 - val_loss: 13905285.0000 - val_mae: 3178.6238\n",
      "Epoch 107/350\n",
      "91/91 - 0s - loss: 8307698.4176 - mae: 2258.0959 - val_loss: 12956949.0000 - val_mae: 2991.1804\n",
      "Epoch 108/350\n",
      "91/91 - 0s - loss: 8196555.8516 - mae: 2245.9082 - val_loss: 12997736.0000 - val_mae: 3036.3835\n",
      "Epoch 109/350\n",
      "91/91 - 0s - loss: 8143087.8681 - mae: 2237.0911 - val_loss: 13036767.0000 - val_mae: 2941.1321\n",
      "Epoch 110/350\n",
      "91/91 - 0s - loss: 8103841.3956 - mae: 2218.0488 - val_loss: 13092451.0000 - val_mae: 2923.2734\n",
      "Epoch 111/350\n",
      "91/91 - 0s - loss: 8184982.5714 - mae: 2236.9167 - val_loss: 11397991.0000 - val_mae: 2863.1484\n",
      "Epoch 112/350\n",
      "91/91 - 0s - loss: 8529397.3297 - mae: 2302.2498 - val_loss: 11402379.0000 - val_mae: 2730.4895\n",
      "Epoch 113/350\n",
      "91/91 - 0s - loss: 8671828.0220 - mae: 2310.0913 - val_loss: 11827939.0000 - val_mae: 2678.1418\n",
      "Epoch 114/350\n",
      "91/91 - 0s - loss: 8381032.5769 - mae: 2234.5576 - val_loss: 11320027.0000 - val_mae: 2693.1887\n",
      "Epoch 115/350\n",
      "91/91 - 0s - loss: 8318226.8462 - mae: 2259.5564 - val_loss: 11323095.0000 - val_mae: 2871.0227\n",
      "Epoch 116/350\n",
      "91/91 - 0s - loss: 8870847.0330 - mae: 2385.6006 - val_loss: 11224700.0000 - val_mae: 2793.6199\n",
      "Epoch 117/350\n",
      "91/91 - 0s - loss: 8793197.4835 - mae: 2352.8071 - val_loss: 11286844.0000 - val_mae: 2666.8618\n",
      "Epoch 118/350\n",
      "91/91 - 0s - loss: 8663194.9176 - mae: 2321.0110 - val_loss: 11031195.0000 - val_mae: 2696.3108\n",
      "Epoch 119/350\n",
      "91/91 - 0s - loss: 8609312.4148 - mae: 2334.3491 - val_loss: 11040327.0000 - val_mae: 2677.2520\n",
      "Epoch 120/350\n",
      "91/91 - 0s - loss: 8434609.7912 - mae: 2292.7769 - val_loss: 11152735.0000 - val_mae: 2655.5515\n",
      "Epoch 121/350\n",
      "91/91 - 0s - loss: 8425423.5769 - mae: 2293.3684 - val_loss: 11123525.0000 - val_mae: 2660.2332\n",
      "Epoch 122/350\n",
      "91/91 - 0s - loss: 8337761.8791 - mae: 2287.1372 - val_loss: 11137700.0000 - val_mae: 2662.8936\n",
      "Epoch 123/350\n",
      "91/91 - 0s - loss: 8444254.7473 - mae: 2311.1428 - val_loss: 11202629.0000 - val_mae: 2683.9500\n",
      "Epoch 124/350\n",
      "91/91 - 0s - loss: 8337331.8736 - mae: 2280.0715 - val_loss: 11656704.0000 - val_mae: 2672.8308\n",
      "Epoch 125/350\n",
      "91/91 - 0s - loss: 8302698.5220 - mae: 2258.7146 - val_loss: 11377031.0000 - val_mae: 2645.5825\n",
      "Epoch 126/350\n",
      "91/91 - 0s - loss: 8239642.4286 - mae: 2296.6143 - val_loss: 11384273.0000 - val_mae: 2721.4617\n",
      "Epoch 127/350\n",
      "91/91 - 0s - loss: 8398050.4945 - mae: 2301.6267 - val_loss: 11204799.0000 - val_mae: 2614.0330\n",
      "Epoch 128/350\n",
      "91/91 - 0s - loss: 8165561.9011 - mae: 2276.8140 - val_loss: 11094814.0000 - val_mae: 2644.1621\n",
      "Epoch 129/350\n",
      "91/91 - 0s - loss: 8051616.8352 - mae: 2267.1003 - val_loss: 11410756.0000 - val_mae: 2631.7322\n",
      "Epoch 130/350\n",
      "91/91 - 0s - loss: 8022316.0714 - mae: 2240.5876 - val_loss: 11723037.0000 - val_mae: 2652.6680\n",
      "Epoch 131/350\n",
      "91/91 - 0s - loss: 8031229.9451 - mae: 2248.1675 - val_loss: 11454600.0000 - val_mae: 2680.7502\n",
      "Epoch 132/350\n",
      "91/91 - 0s - loss: 8317881.3846 - mae: 2331.5452 - val_loss: 12015403.0000 - val_mae: 2852.3457\n",
      "Epoch 133/350\n",
      "91/91 - 0s - loss: 8336400.4066 - mae: 2318.9128 - val_loss: 11976104.0000 - val_mae: 2680.6323\n",
      "Epoch 134/350\n",
      "91/91 - 0s - loss: 7849861.3846 - mae: 2222.9248 - val_loss: 11646373.0000 - val_mae: 2634.5930\n",
      "Epoch 135/350\n",
      "91/91 - 0s - loss: 7949525.4505 - mae: 2249.8135 - val_loss: 11790777.0000 - val_mae: 2707.5361\n",
      "Epoch 136/350\n",
      "91/91 - 0s - loss: 7694773.6593 - mae: 2210.1814 - val_loss: 11810141.0000 - val_mae: 2656.3679\n",
      "Epoch 137/350\n",
      "91/91 - 0s - loss: 7684545.5165 - mae: 2202.8330 - val_loss: 12267624.0000 - val_mae: 2714.1765\n",
      "Epoch 138/350\n",
      "91/91 - 0s - loss: 7826420.3736 - mae: 2213.6333 - val_loss: 11702437.0000 - val_mae: 2633.6790\n",
      "Epoch 139/350\n",
      "91/91 - 0s - loss: 7573060.5604 - mae: 2180.8657 - val_loss: 11718255.0000 - val_mae: 2620.0232\n",
      "Epoch 140/350\n",
      "91/91 - 0s - loss: 7563209.3132 - mae: 2185.7891 - val_loss: 11686333.0000 - val_mae: 2605.1799\n",
      "Epoch 141/350\n",
      "91/91 - 0s - loss: 7798539.8242 - mae: 2232.6140 - val_loss: 11882280.0000 - val_mae: 2631.8923\n",
      "Epoch 142/350\n",
      "91/91 - 0s - loss: 7526639.9121 - mae: 2168.4812 - val_loss: 11648843.0000 - val_mae: 2587.6711\n",
      "Epoch 143/350\n",
      "91/91 - 0s - loss: 7746020.2582 - mae: 2218.7388 - val_loss: 11629192.0000 - val_mae: 2595.4961\n",
      "Epoch 144/350\n",
      "91/91 - 0s - loss: 7552990.4780 - mae: 2184.1526 - val_loss: 12335709.0000 - val_mae: 2731.2166\n",
      "Epoch 145/350\n",
      "91/91 - 0s - loss: 7607929.7967 - mae: 2190.2056 - val_loss: 11715435.0000 - val_mae: 2587.4980\n",
      "Epoch 146/350\n",
      "91/91 - 0s - loss: 7701494.1868 - mae: 2211.8188 - val_loss: 11703745.0000 - val_mae: 2592.2341\n",
      "Epoch 147/350\n",
      "91/91 - 0s - loss: 7340033.6154 - mae: 2163.8701 - val_loss: 12338279.0000 - val_mae: 2753.0430\n",
      "Epoch 148/350\n",
      "91/91 - 0s - loss: 7580541.5385 - mae: 2170.3501 - val_loss: 11757877.0000 - val_mae: 2611.0100\n",
      "Epoch 149/350\n",
      "91/91 - 0s - loss: 7386081.1538 - mae: 2139.3528 - val_loss: 11829655.0000 - val_mae: 2652.9475\n",
      "Epoch 150/350\n",
      "91/91 - 0s - loss: 7521741.6813 - mae: 2183.3164 - val_loss: 11677480.0000 - val_mae: 2616.9167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/350\n",
      "91/91 - 0s - loss: 7329164.4835 - mae: 2137.4131 - val_loss: 11693201.0000 - val_mae: 2625.2888\n",
      "Epoch 152/350\n",
      "91/91 - 0s - loss: 7504653.3681 - mae: 2148.3906 - val_loss: 11603029.0000 - val_mae: 2576.0811\n",
      "Epoch 153/350\n",
      "91/91 - 0s - loss: 7276877.1813 - mae: 2143.4192 - val_loss: 11706953.0000 - val_mae: 2614.7400\n",
      "Epoch 154/350\n",
      "91/91 - 0s - loss: 7475528.5055 - mae: 2148.9663 - val_loss: 11697901.0000 - val_mae: 2570.7109\n",
      "Epoch 155/350\n",
      "91/91 - 0s - loss: 7436493.0604 - mae: 2167.2812 - val_loss: 11781269.0000 - val_mae: 2551.6963\n",
      "Epoch 156/350\n",
      "91/91 - 0s - loss: 7228677.7253 - mae: 2104.7751 - val_loss: 12137261.0000 - val_mae: 2714.5000\n",
      "Epoch 157/350\n",
      "91/91 - 0s - loss: 7227285.2802 - mae: 2101.7581 - val_loss: 11748659.0000 - val_mae: 2525.9199\n",
      "Epoch 158/350\n",
      "91/91 - 0s - loss: 7129888.8297 - mae: 2110.7175 - val_loss: 11850165.0000 - val_mae: 2545.9785\n",
      "Epoch 159/350\n",
      "91/91 - 0s - loss: 7313511.5165 - mae: 2152.4766 - val_loss: 11835483.0000 - val_mae: 2549.6912\n",
      "Epoch 160/350\n",
      "91/91 - 0s - loss: 7361407.0165 - mae: 2138.6553 - val_loss: 12183164.0000 - val_mae: 2738.0352\n",
      "Epoch 161/350\n",
      "91/91 - 0s - loss: 7300076.0220 - mae: 2132.3572 - val_loss: 11900313.0000 - val_mae: 2557.6921\n",
      "Epoch 162/350\n",
      "91/91 - 0s - loss: 7391668.3846 - mae: 2150.4536 - val_loss: 11736701.0000 - val_mae: 2611.6079\n",
      "Epoch 163/350\n",
      "91/91 - 0s - loss: 7074936.2995 - mae: 2086.5276 - val_loss: 12055640.0000 - val_mae: 2739.6975\n",
      "Epoch 164/350\n",
      "91/91 - 0s - loss: 7273333.4176 - mae: 2100.5103 - val_loss: 11709577.0000 - val_mae: 2539.4746\n",
      "Epoch 165/350\n",
      "91/91 - 0s - loss: 7334329.4341 - mae: 2128.3748 - val_loss: 12245249.0000 - val_mae: 2665.1492\n",
      "Epoch 166/350\n",
      "91/91 - 0s - loss: 7156458.8352 - mae: 2110.5842 - val_loss: 12267951.0000 - val_mae: 2784.2161\n",
      "Epoch 167/350\n",
      "91/91 - 0s - loss: 7348110.8132 - mae: 2107.9629 - val_loss: 11920213.0000 - val_mae: 2684.2046\n",
      "Epoch 168/350\n",
      "91/91 - 0s - loss: 7148499.8571 - mae: 2111.6653 - val_loss: 12263273.0000 - val_mae: 2648.8215\n",
      "Epoch 169/350\n",
      "91/91 - 0s - loss: 7184097.8352 - mae: 2123.0935 - val_loss: 11943491.0000 - val_mae: 2694.6731\n",
      "Epoch 170/350\n",
      "91/91 - 0s - loss: 7075811.0220 - mae: 2083.9331 - val_loss: 12042105.0000 - val_mae: 2735.0974\n",
      "Epoch 171/350\n",
      "91/91 - 0s - loss: 7034313.5220 - mae: 2098.4829 - val_loss: 11834076.0000 - val_mae: 2556.0911\n",
      "Epoch 172/350\n",
      "91/91 - 0s - loss: 7068514.1813 - mae: 2098.1223 - val_loss: 11732068.0000 - val_mae: 2577.4514\n",
      "Epoch 173/350\n",
      "91/91 - 0s - loss: 7184975.5330 - mae: 2114.9128 - val_loss: 11951173.0000 - val_mae: 2738.6580\n",
      "Epoch 174/350\n",
      "91/91 - 0s - loss: 7132677.4231 - mae: 2094.4507 - val_loss: 11824245.0000 - val_mae: 2578.2002\n",
      "Epoch 175/350\n",
      "91/91 - 0s - loss: 6984891.1868 - mae: 2078.3823 - val_loss: 11787773.0000 - val_mae: 2654.4460\n",
      "Epoch 176/350\n",
      "91/91 - 0s - loss: 6965091.6484 - mae: 2060.7124 - val_loss: 11849599.0000 - val_mae: 2715.3379\n",
      "Epoch 177/350\n",
      "91/91 - 0s - loss: 7011151.3352 - mae: 2071.1160 - val_loss: 11698129.0000 - val_mae: 2620.9060\n",
      "Epoch 178/350\n",
      "91/91 - 0s - loss: 6898249.7857 - mae: 2073.2986 - val_loss: 11919187.0000 - val_mae: 2582.7422\n",
      "Epoch 179/350\n",
      "91/91 - 0s - loss: 7010565.6703 - mae: 2085.7178 - val_loss: 11851491.0000 - val_mae: 2701.5383\n",
      "Epoch 180/350\n",
      "91/91 - 0s - loss: 7105115.2253 - mae: 2082.5173 - val_loss: 11741452.0000 - val_mae: 2639.6926\n",
      "Epoch 181/350\n",
      "91/91 - 0s - loss: 6968935.0549 - mae: 2075.8831 - val_loss: 11814101.0000 - val_mae: 2699.5164\n",
      "Epoch 182/350\n",
      "91/91 - 0s - loss: 7304313.3626 - mae: 2121.7544 - val_loss: 11855087.0000 - val_mae: 2580.1272\n",
      "Epoch 183/350\n",
      "91/91 - 0s - loss: 6886706.3516 - mae: 2052.3533 - val_loss: 12110529.0000 - val_mae: 2803.9055\n",
      "Epoch 184/350\n",
      "91/91 - 0s - loss: 7047103.6484 - mae: 2059.4839 - val_loss: 11703711.0000 - val_mae: 2623.6177\n",
      "Epoch 185/350\n",
      "91/91 - 0s - loss: 6864425.4286 - mae: 2079.0525 - val_loss: 11880205.0000 - val_mae: 2581.5930\n",
      "Epoch 186/350\n",
      "91/91 - 0s - loss: 6937757.2582 - mae: 2081.0000 - val_loss: 11848783.0000 - val_mae: 2728.1616\n",
      "Epoch 187/350\n",
      "91/91 - 0s - loss: 6937345.1264 - mae: 2049.1084 - val_loss: 11920720.0000 - val_mae: 2738.2732\n",
      "Epoch 188/350\n",
      "91/91 - 0s - loss: 6824576.1429 - mae: 2044.6555 - val_loss: 11893055.0000 - val_mae: 2591.9290\n",
      "Epoch 189/350\n",
      "91/91 - 0s - loss: 6923155.4176 - mae: 2070.8955 - val_loss: 11708060.0000 - val_mae: 2636.7104\n",
      "Epoch 190/350\n",
      "91/91 - 0s - loss: 6875068.6868 - mae: 2052.9214 - val_loss: 11767692.0000 - val_mae: 2728.2947\n",
      "Epoch 191/350\n",
      "91/91 - 0s - loss: 7008116.7912 - mae: 2050.6521 - val_loss: 11782184.0000 - val_mae: 2729.0325\n",
      "Epoch 192/350\n",
      "91/91 - 0s - loss: 6771013.3187 - mae: 2030.5242 - val_loss: 11883975.0000 - val_mae: 2565.5276\n",
      "Epoch 193/350\n",
      "91/91 - 0s - loss: 7188484.1319 - mae: 2111.9724 - val_loss: 11763857.0000 - val_mae: 2606.0703\n",
      "Epoch 194/350\n",
      "91/91 - 0s - loss: 7018312.4725 - mae: 2089.5508 - val_loss: 12172432.0000 - val_mae: 2832.0762\n",
      "Epoch 195/350\n",
      "91/91 - 0s - loss: 6861291.1429 - mae: 2050.6902 - val_loss: 11857675.0000 - val_mae: 2593.6560\n",
      "Epoch 196/350\n",
      "91/91 - 0s - loss: 7010929.5440 - mae: 2083.6975 - val_loss: 11742459.0000 - val_mae: 2647.4534\n",
      "Epoch 197/350\n",
      "91/91 - 0s - loss: 6818881.6868 - mae: 2047.8860 - val_loss: 11711231.0000 - val_mae: 2648.7927\n",
      "Epoch 198/350\n",
      "91/91 - 0s - loss: 7116285.8297 - mae: 2102.0007 - val_loss: 11726029.0000 - val_mae: 2694.4036\n",
      "Epoch 199/350\n",
      "91/91 - 0s - loss: 6840755.6154 - mae: 2014.5347 - val_loss: 11882264.0000 - val_mae: 2774.2043\n",
      "Epoch 200/350\n",
      "91/91 - 0s - loss: 6770966.6813 - mae: 2042.8702 - val_loss: 11849321.0000 - val_mae: 2590.9761\n",
      "Epoch 201/350\n",
      "91/91 - 0s - loss: 6987564.5330 - mae: 2086.2373 - val_loss: 11671233.0000 - val_mae: 2623.7378\n",
      "Epoch 202/350\n",
      "91/91 - 0s - loss: 7152097.2967 - mae: 2070.9402 - val_loss: 11931312.0000 - val_mae: 2798.2148\n",
      "Epoch 203/350\n",
      "91/91 - 0s - loss: 6797875.8681 - mae: 2023.5127 - val_loss: 11868627.0000 - val_mae: 2571.7632\n",
      "Epoch 204/350\n",
      "91/91 - 0s - loss: 6874158.4066 - mae: 2075.1167 - val_loss: 11522800.0000 - val_mae: 2618.2100\n",
      "Epoch 205/350\n",
      "91/91 - 0s - loss: 6683707.9396 - mae: 2003.3640 - val_loss: 11983715.0000 - val_mae: 2780.2546\n",
      "Epoch 206/350\n",
      "91/91 - 0s - loss: 6743176.0879 - mae: 2012.4976 - val_loss: 11640999.0000 - val_mae: 2566.4055\n",
      "Epoch 207/350\n",
      "91/91 - 0s - loss: 6698641.1813 - mae: 2014.0250 - val_loss: 11629400.0000 - val_mae: 2566.0518\n",
      "Epoch 208/350\n",
      "91/91 - 0s - loss: 6864392.4560 - mae: 2055.7451 - val_loss: 11561153.0000 - val_mae: 2684.5859\n",
      "Epoch 209/350\n",
      "91/91 - 0s - loss: 6682090.5165 - mae: 1996.4982 - val_loss: 11655917.0000 - val_mae: 2734.8220\n",
      "Epoch 210/350\n",
      "91/91 - 0s - loss: 6698657.7912 - mae: 2001.5461 - val_loss: 11466168.0000 - val_mae: 2555.0156\n",
      "Epoch 211/350\n",
      "91/91 - 0s - loss: 6831723.8352 - mae: 2043.1116 - val_loss: 11395651.0000 - val_mae: 2605.4719\n",
      "Epoch 212/350\n",
      "91/91 - 0s - loss: 6711477.9231 - mae: 2005.7136 - val_loss: 11473304.0000 - val_mae: 2541.4072\n",
      "Epoch 213/350\n",
      "91/91 - 0s - loss: 6565076.9396 - mae: 1990.5942 - val_loss: 11579023.0000 - val_mae: 2714.1650\n",
      "Epoch 214/350\n",
      "91/91 - 0s - loss: 6873562.0110 - mae: 2010.1451 - val_loss: 11614275.0000 - val_mae: 2699.6699\n",
      "Epoch 215/350\n",
      "91/91 - 0s - loss: 6738604.8901 - mae: 2006.1903 - val_loss: 11893045.0000 - val_mae: 2569.8469\n",
      "Epoch 216/350\n",
      "91/91 - 0s - loss: 6641057.7802 - mae: 2009.2881 - val_loss: 11718933.0000 - val_mae: 2735.2461\n",
      "Epoch 217/350\n",
      "91/91 - 0s - loss: 7189570.0879 - mae: 2066.8049 - val_loss: 11785784.0000 - val_mae: 2777.1455\n",
      "Epoch 218/350\n",
      "91/91 - 0s - loss: 7048609.7198 - mae: 2052.5688 - val_loss: 12533677.0000 - val_mae: 2738.2546\n",
      "Epoch 219/350\n",
      "91/91 - 0s - loss: 7012218.9615 - mae: 2064.0369 - val_loss: 11929923.0000 - val_mae: 2819.1257\n",
      "Epoch 220/350\n",
      "91/91 - 0s - loss: 6939528.0220 - mae: 2008.8910 - val_loss: 11620119.0000 - val_mae: 2754.5452\n",
      "Epoch 221/350\n",
      "91/91 - 0s - loss: 6844896.2473 - mae: 2023.7788 - val_loss: 11738039.0000 - val_mae: 2586.1943\n",
      "Epoch 222/350\n",
      "91/91 - 0s - loss: 6730816.0440 - mae: 2011.9595 - val_loss: 11702955.0000 - val_mae: 2745.6960\n",
      "Epoch 223/350\n",
      "91/91 - 0s - loss: 6791711.8571 - mae: 1994.9938 - val_loss: 11517865.0000 - val_mae: 2685.9827\n",
      "Epoch 224/350\n",
      "91/91 - 0s - loss: 6653624.9066 - mae: 2012.6866 - val_loss: 11691307.0000 - val_mae: 2552.9385\n",
      "Epoch 225/350\n",
      "91/91 - 0s - loss: 6684279.5440 - mae: 2025.4458 - val_loss: 11490184.0000 - val_mae: 2708.7307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/350\n",
      "91/91 - 0s - loss: 6965560.5824 - mae: 1958.9586 - val_loss: 12038723.0000 - val_mae: 2834.2441\n",
      "Epoch 227/350\n",
      "91/91 - 0s - loss: 6794360.3846 - mae: 2017.4620 - val_loss: 12028445.0000 - val_mae: 2592.9485\n",
      "Epoch 228/350\n",
      "91/91 - 0s - loss: 6895631.7912 - mae: 2064.0417 - val_loss: 11551472.0000 - val_mae: 2737.3142\n",
      "Epoch 229/350\n",
      "91/91 - 0s - loss: 6880807.7308 - mae: 2017.6724 - val_loss: 11329693.0000 - val_mae: 2644.6458\n",
      "Epoch 230/350\n",
      "91/91 - 0s - loss: 7055689.5330 - mae: 2053.2991 - val_loss: 11525757.0000 - val_mae: 2538.7822\n",
      "Epoch 231/350\n",
      "91/91 - 0s - loss: 6480700.5165 - mae: 1983.4805 - val_loss: 12205477.0000 - val_mae: 2870.8484\n",
      "Epoch 232/350\n",
      "91/91 - 0s - loss: 6816956.8571 - mae: 1982.7904 - val_loss: 11514435.0000 - val_mae: 2598.5254\n",
      "Epoch 233/350\n",
      "91/91 - 0s - loss: 6569156.9670 - mae: 1990.9680 - val_loss: 11604828.0000 - val_mae: 2556.0574\n",
      "Epoch 234/350\n",
      "91/91 - 0s - loss: 6578310.0110 - mae: 1992.6377 - val_loss: 11463336.0000 - val_mae: 2674.8645\n",
      "Epoch 235/350\n",
      "91/91 - 0s - loss: 6486428.5495 - mae: 1957.8604 - val_loss: 11379581.0000 - val_mae: 2616.4414\n",
      "Epoch 236/350\n",
      "91/91 - 0s - loss: 6507375.9615 - mae: 1967.4591 - val_loss: 11347619.0000 - val_mae: 2658.8611\n",
      "Epoch 237/350\n",
      "91/91 - 0s - loss: 6554478.8022 - mae: 1973.9955 - val_loss: 11349139.0000 - val_mae: 2583.3640\n",
      "Epoch 238/350\n",
      "91/91 - 0s - loss: 6517983.5934 - mae: 1974.9218 - val_loss: 11300989.0000 - val_mae: 2603.1545\n",
      "Epoch 239/350\n",
      "91/91 - 0s - loss: 6454442.1593 - mae: 1957.1799 - val_loss: 11505377.0000 - val_mae: 2723.6394\n",
      "Epoch 240/350\n",
      "91/91 - 0s - loss: 6746782.4615 - mae: 2033.9933 - val_loss: 11437109.0000 - val_mae: 2625.1470\n",
      "Epoch 241/350\n",
      "91/91 - 0s - loss: 6444162.4121 - mae: 1953.3115 - val_loss: 11510221.0000 - val_mae: 2711.1646\n",
      "Epoch 242/350\n",
      "91/91 - 0s - loss: 6689535.4286 - mae: 1990.5134 - val_loss: 11406309.0000 - val_mae: 2686.5784\n",
      "Epoch 243/350\n",
      "91/91 - 0s - loss: 6598893.0440 - mae: 1967.9420 - val_loss: 11650952.0000 - val_mae: 2567.8967\n",
      "Epoch 244/350\n",
      "91/91 - 0s - loss: 6491457.5549 - mae: 1975.0920 - val_loss: 11589368.0000 - val_mae: 2783.8640\n",
      "Epoch 245/350\n",
      "91/91 - 0s - loss: 6701281.2418 - mae: 1965.3058 - val_loss: 11580668.0000 - val_mae: 2773.0227\n",
      "Epoch 246/350\n",
      "91/91 - 0s - loss: 6357098.9231 - mae: 1936.7258 - val_loss: 11684808.0000 - val_mae: 2551.8098\n",
      "Epoch 247/350\n",
      "91/91 - 0s - loss: 7123342.8736 - mae: 2111.6274 - val_loss: 11420796.0000 - val_mae: 2553.8284\n",
      "Epoch 248/350\n",
      "91/91 - 0s - loss: 7175809.0549 - mae: 2036.6600 - val_loss: 12692053.0000 - val_mae: 2947.7041\n",
      "Epoch 249/350\n",
      "91/91 - 0s - loss: 6874424.0879 - mae: 1927.0577 - val_loss: 12088667.0000 - val_mae: 2624.4431\n",
      "Epoch 250/350\n",
      "91/91 - 0s - loss: 6895704.3846 - mae: 2059.4651 - val_loss: 11428715.0000 - val_mae: 2686.3574\n",
      "Epoch 251/350\n",
      "91/91 - 0s - loss: 6395716.5440 - mae: 1935.2150 - val_loss: 11691605.0000 - val_mae: 2766.5769\n",
      "Epoch 252/350\n",
      "91/91 - 0s - loss: 6497218.1648 - mae: 1945.3893 - val_loss: 11432213.0000 - val_mae: 2586.4680\n",
      "Epoch 253/350\n",
      "91/91 - 0s - loss: 6409244.9560 - mae: 1957.1825 - val_loss: 11597975.0000 - val_mae: 2536.5520\n",
      "Epoch 254/350\n",
      "91/91 - 0s - loss: 6500094.9011 - mae: 1958.8865 - val_loss: 11498536.0000 - val_mae: 2725.8655\n",
      "Epoch 255/350\n",
      "91/91 - 0s - loss: 6544246.8022 - mae: 1944.6428 - val_loss: 11333307.0000 - val_mae: 2646.3704\n",
      "Epoch 256/350\n",
      "91/91 - 0s - loss: 6559691.3516 - mae: 1982.6471 - val_loss: 11454509.0000 - val_mae: 2529.7615\n",
      "Epoch 257/350\n",
      "91/91 - 0s - loss: 6306844.4945 - mae: 1929.6874 - val_loss: 11779805.0000 - val_mae: 2805.7874\n",
      "Epoch 258/350\n",
      "91/91 - 0s - loss: 6625758.2363 - mae: 1947.3589 - val_loss: 11379519.0000 - val_mae: 2699.3723\n",
      "Epoch 259/350\n",
      "91/91 - 0s - loss: 6526244.0769 - mae: 1974.5676 - val_loss: 11603477.0000 - val_mae: 2531.8638\n",
      "Epoch 260/350\n",
      "91/91 - 0s - loss: 6679375.4890 - mae: 1977.7655 - val_loss: 11893379.0000 - val_mae: 2833.6609\n",
      "Epoch 261/350\n",
      "91/91 - 0s - loss: 6704714.9451 - mae: 1971.5912 - val_loss: 11294324.0000 - val_mae: 2622.2024\n",
      "Epoch 262/350\n",
      "91/91 - 0s - loss: 6429345.5385 - mae: 1953.0704 - val_loss: 11389357.0000 - val_mae: 2538.3848\n",
      "Epoch 263/350\n",
      "91/91 - 0s - loss: 6822054.2692 - mae: 2004.1427 - val_loss: 11668181.0000 - val_mae: 2763.1926\n",
      "Epoch 264/350\n",
      "91/91 - 0s - loss: 6715570.1868 - mae: 2009.8060 - val_loss: 11612829.0000 - val_mae: 2514.9641\n",
      "Epoch 265/350\n",
      "91/91 - 0s - loss: 6316473.7363 - mae: 1929.2889 - val_loss: 11680437.0000 - val_mae: 2785.8230\n",
      "Epoch 266/350\n",
      "91/91 - 0s - loss: 6781281.9176 - mae: 1978.3378 - val_loss: 11451767.0000 - val_mae: 2728.1565\n",
      "Epoch 267/350\n",
      "91/91 - 0s - loss: 7283373.7143 - mae: 2117.0183 - val_loss: 11749075.0000 - val_mae: 2556.6118\n",
      "Epoch 268/350\n",
      "91/91 - 0s - loss: 6336836.9533 - mae: 1944.4059 - val_loss: 12428693.0000 - val_mae: 2905.7048\n",
      "Epoch 269/350\n",
      "91/91 - 0s - loss: 6915560.3407 - mae: 1982.5917 - val_loss: 11333483.0000 - val_mae: 2612.4180\n",
      "Epoch 270/350\n",
      "91/91 - 0s - loss: 6411832.6703 - mae: 1941.5525 - val_loss: 11550396.0000 - val_mae: 2511.7236\n",
      "Epoch 271/350\n",
      "91/91 - 0s - loss: 6426894.0220 - mae: 1941.0562 - val_loss: 11657071.0000 - val_mae: 2758.6653\n",
      "Epoch 272/350\n",
      "91/91 - 0s - loss: 6419394.8681 - mae: 1934.6875 - val_loss: 11326541.0000 - val_mae: 2633.8049\n",
      "Epoch 273/350\n",
      "91/91 - 0s - loss: 6311726.3022 - mae: 1933.3057 - val_loss: 11323759.0000 - val_mae: 2556.4724\n",
      "Epoch 274/350\n",
      "91/91 - 0s - loss: 6306378.0962 - mae: 1917.0864 - val_loss: 11293465.0000 - val_mae: 2696.3440\n",
      "Epoch 275/350\n",
      "91/91 - 0s - loss: 6326206.1868 - mae: 1908.8722 - val_loss: 11254088.0000 - val_mae: 2656.2573\n",
      "Epoch 276/350\n",
      "91/91 - 0s - loss: 6378222.0989 - mae: 1906.6968 - val_loss: 11257115.0000 - val_mae: 2637.8518\n",
      "Epoch 277/350\n",
      "91/91 - 0s - loss: 6376430.3132 - mae: 1934.2313 - val_loss: 11347120.0000 - val_mae: 2539.2830\n",
      "Epoch 278/350\n",
      "91/91 - 0s - loss: 6457166.7967 - mae: 1929.5021 - val_loss: 11665197.0000 - val_mae: 2789.2605\n",
      "Epoch 279/350\n",
      "91/91 - 0s - loss: 6338307.9121 - mae: 1918.5448 - val_loss: 11427647.0000 - val_mae: 2526.4070\n",
      "Epoch 280/350\n",
      "91/91 - 0s - loss: 6348659.0632 - mae: 1938.4595 - val_loss: 11304877.0000 - val_mae: 2638.7373\n",
      "Epoch 281/350\n",
      "91/91 - 0s - loss: 6305951.5989 - mae: 1893.3851 - val_loss: 11498563.0000 - val_mae: 2739.3660\n",
      "Epoch 282/350\n",
      "91/91 - 0s - loss: 6499323.2857 - mae: 1944.0050 - val_loss: 11272773.0000 - val_mae: 2595.7588\n",
      "Epoch 283/350\n",
      "91/91 - 0s - loss: 6235966.5879 - mae: 1901.5670 - val_loss: 11350773.0000 - val_mae: 2712.4407\n",
      "Epoch 284/350\n",
      "91/91 - 0s - loss: 6401424.0714 - mae: 1910.8691 - val_loss: 11304769.0000 - val_mae: 2669.7881\n",
      "Epoch 285/350\n",
      "91/91 - 0s - loss: 6281531.8297 - mae: 1916.2698 - val_loss: 11305356.0000 - val_mae: 2567.4321\n",
      "Epoch 286/350\n",
      "91/91 - 0s - loss: 6370962.4451 - mae: 1904.0043 - val_loss: 11388909.0000 - val_mae: 2666.2339\n",
      "Epoch 287/350\n",
      "91/91 - 0s - loss: 6199628.5110 - mae: 1887.3928 - val_loss: 11335617.0000 - val_mae: 2627.8054\n",
      "Epoch 288/350\n",
      "91/91 - 0s - loss: 6275444.3352 - mae: 1912.6807 - val_loss: 11317185.0000 - val_mae: 2659.8577\n",
      "Epoch 289/350\n",
      "91/91 - 0s - loss: 6365205.8187 - mae: 1931.6740 - val_loss: 11360941.0000 - val_mae: 2716.5432\n",
      "Epoch 290/350\n",
      "91/91 - 0s - loss: 6191652.5714 - mae: 1884.8214 - val_loss: 11365608.0000 - val_mae: 2516.2825\n",
      "Epoch 291/350\n",
      "91/91 - 0s - loss: 6350559.8352 - mae: 1931.5798 - val_loss: 11349184.0000 - val_mae: 2722.5771\n",
      "Epoch 292/350\n",
      "91/91 - 0s - loss: 6356748.3516 - mae: 1891.3164 - val_loss: 11233984.0000 - val_mae: 2599.7031\n",
      "Epoch 293/350\n",
      "91/91 - 0s - loss: 6452085.3956 - mae: 1992.7433 - val_loss: 11354400.0000 - val_mae: 2525.9915\n",
      "Epoch 294/350\n",
      "91/91 - 0s - loss: 6062003.1813 - mae: 1875.5284 - val_loss: 11738917.0000 - val_mae: 2833.1023\n",
      "Epoch 295/350\n",
      "91/91 - 0s - loss: 6475474.4011 - mae: 1900.1722 - val_loss: 11133146.0000 - val_mae: 2635.5042\n",
      "Epoch 296/350\n",
      "91/91 - 0s - loss: 6243107.6374 - mae: 1922.5481 - val_loss: 11241811.0000 - val_mae: 2518.4661\n",
      "Epoch 297/350\n",
      "91/91 - 0s - loss: 6200202.8956 - mae: 1902.0100 - val_loss: 11360971.0000 - val_mae: 2715.4470\n",
      "Epoch 298/350\n",
      "91/91 - 0s - loss: 6624656.6538 - mae: 1939.3510 - val_loss: 11307724.0000 - val_mae: 2649.7097\n",
      "Epoch 299/350\n",
      "91/91 - 0s - loss: 6048269.1429 - mae: 1903.4017 - val_loss: 11988451.0000 - val_mae: 2605.5312\n",
      "Epoch 300/350\n",
      "91/91 - 0s - loss: 6619127.6264 - mae: 1991.2445 - val_loss: 11299640.0000 - val_mae: 2732.4258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 301/350\n",
      "91/91 - 0s - loss: 6368317.0275 - mae: 1912.5148 - val_loss: 11060023.0000 - val_mae: 2660.7229\n",
      "Epoch 302/350\n",
      "91/91 - 0s - loss: 6230909.5604 - mae: 1901.7892 - val_loss: 11100877.0000 - val_mae: 2649.5261\n",
      "Epoch 303/350\n",
      "91/91 - 0s - loss: 6219709.0440 - mae: 1892.1099 - val_loss: 11148837.0000 - val_mae: 2658.6304\n",
      "Epoch 304/350\n",
      "91/91 - 0s - loss: 6133135.4560 - mae: 1871.7546 - val_loss: 11149963.0000 - val_mae: 2662.3052\n",
      "Epoch 305/350\n",
      "91/91 - 0s - loss: 6160468.9505 - mae: 1873.3197 - val_loss: 11155213.0000 - val_mae: 2642.6514\n",
      "Epoch 306/350\n",
      "91/91 - 0s - loss: 6119118.4835 - mae: 1872.2603 - val_loss: 11223711.0000 - val_mae: 2592.5920\n",
      "Epoch 307/350\n",
      "91/91 - 0s - loss: 6272981.2527 - mae: 1933.0009 - val_loss: 11187175.0000 - val_mae: 2612.0071\n",
      "Epoch 308/350\n",
      "91/91 - 0s - loss: 6451600.6758 - mae: 1936.5447 - val_loss: 11463405.0000 - val_mae: 2754.9883\n",
      "Epoch 309/350\n",
      "91/91 - 0s - loss: 6154057.0879 - mae: 1884.9733 - val_loss: 11736499.0000 - val_mae: 2552.4219\n",
      "Epoch 310/350\n",
      "91/91 - 0s - loss: 6370248.1209 - mae: 1992.2102 - val_loss: 11366128.0000 - val_mae: 2720.7168\n",
      "Epoch 311/350\n",
      "91/91 - 0s - loss: 6581779.2198 - mae: 1927.7361 - val_loss: 11195011.0000 - val_mae: 2591.4619\n",
      "Epoch 312/350\n",
      "91/91 - 0s - loss: 6890027.5549 - mae: 2085.5034 - val_loss: 11313020.0000 - val_mae: 2506.0847\n",
      "Epoch 313/350\n",
      "91/91 - 0s - loss: 6496468.9615 - mae: 1944.2205 - val_loss: 12037352.0000 - val_mae: 2888.9629\n",
      "Epoch 314/350\n",
      "91/91 - 0s - loss: 6705495.8846 - mae: 1987.8990 - val_loss: 11409213.0000 - val_mae: 2514.3389\n",
      "Epoch 315/350\n",
      "91/91 - 0s - loss: 6556617.1209 - mae: 1945.9399 - val_loss: 11332559.0000 - val_mae: 2755.4319\n",
      "Epoch 316/350\n",
      "91/91 - 0s - loss: 6120871.7473 - mae: 1865.1803 - val_loss: 11286395.0000 - val_mae: 2517.8723\n",
      "Epoch 317/350\n",
      "91/91 - 0s - loss: 6491852.1648 - mae: 1993.0873 - val_loss: 11141034.0000 - val_mae: 2565.4045\n",
      "Epoch 318/350\n",
      "91/91 - 0s - loss: 7225992.1209 - mae: 1991.6099 - val_loss: 11698872.0000 - val_mae: 2819.2981\n",
      "Epoch 319/350\n",
      "91/91 - 0s - loss: 6554933.5824 - mae: 1950.7383 - val_loss: 11786779.0000 - val_mae: 2590.1423\n",
      "Epoch 320/350\n",
      "91/91 - 0s - loss: 6282947.1593 - mae: 1934.2589 - val_loss: 11439463.0000 - val_mae: 2757.4148\n",
      "Epoch 321/350\n",
      "91/91 - 0s - loss: 6184219.2527 - mae: 1866.8599 - val_loss: 11134512.0000 - val_mae: 2652.2336\n",
      "Epoch 322/350\n",
      "91/91 - 0s - loss: 6065286.3297 - mae: 1878.9819 - val_loss: 11090551.0000 - val_mae: 2528.6633\n",
      "Epoch 323/350\n",
      "91/91 - 0s - loss: 6090560.4725 - mae: 1882.8965 - val_loss: 11138261.0000 - val_mae: 2681.2097\n",
      "Epoch 324/350\n",
      "91/91 - 0s - loss: 6143615.8407 - mae: 1865.0450 - val_loss: 11100187.0000 - val_mae: 2620.3655\n",
      "Epoch 325/350\n",
      "91/91 - 0s - loss: 6075290.8462 - mae: 1881.0938 - val_loss: 11159430.0000 - val_mae: 2535.7581\n",
      "Epoch 326/350\n",
      "91/91 - 0s - loss: 6210234.7418 - mae: 1903.0059 - val_loss: 11157347.0000 - val_mae: 2639.3728\n",
      "Epoch 327/350\n",
      "91/91 - 0s - loss: 6273848.8681 - mae: 1852.6415 - val_loss: 11519864.0000 - val_mae: 2747.6875\n",
      "Epoch 328/350\n",
      "91/91 - 0s - loss: 6319982.4725 - mae: 1921.3510 - val_loss: 11475328.0000 - val_mae: 2500.4431\n",
      "Epoch 329/350\n",
      "91/91 - 0s - loss: 6378968.8571 - mae: 1901.8801 - val_loss: 11386840.0000 - val_mae: 2772.4910\n",
      "Epoch 330/350\n",
      "91/91 - 0s - loss: 6146976.4231 - mae: 1875.0988 - val_loss: 11095811.0000 - val_mae: 2516.9714\n",
      "Epoch 331/350\n",
      "91/91 - 0s - loss: 6161070.8791 - mae: 1913.3025 - val_loss: 11052457.0000 - val_mae: 2638.2402\n",
      "Epoch 332/350\n",
      "91/91 - 0s - loss: 6834949.4066 - mae: 1956.1030 - val_loss: 11449644.0000 - val_mae: 2761.3508\n",
      "Epoch 333/350\n",
      "91/91 - 0s - loss: 5949839.1868 - mae: 1828.8314 - val_loss: 12336527.0000 - val_mae: 2738.2742\n",
      "Epoch 334/350\n",
      "91/91 - 0s - loss: 6499031.9725 - mae: 1983.6659 - val_loss: 11835928.0000 - val_mae: 2783.2275\n",
      "Epoch 335/350\n",
      "91/91 - 0s - loss: 6373111.6154 - mae: 1889.3513 - val_loss: 11434223.0000 - val_mae: 2695.8777\n",
      "Epoch 336/350\n",
      "91/91 - 0s - loss: 6223409.4396 - mae: 1892.1931 - val_loss: 11185320.0000 - val_mae: 2519.5149\n",
      "Epoch 337/350\n",
      "91/91 - 0s - loss: 6492575.5275 - mae: 1976.2892 - val_loss: 10932369.0000 - val_mae: 2644.1072\n",
      "Epoch 338/350\n",
      "91/91 - 0s - loss: 6564595.1648 - mae: 1899.9219 - val_loss: 11206451.0000 - val_mae: 2724.4160\n",
      "Epoch 339/350\n",
      "91/91 - 0s - loss: 6732941.2198 - mae: 1991.7362 - val_loss: 11154425.0000 - val_mae: 2471.2937\n",
      "Epoch 340/350\n",
      "91/91 - 0s - loss: 6432667.6538 - mae: 1903.1453 - val_loss: 11599259.0000 - val_mae: 2845.7529\n",
      "Epoch 341/350\n",
      "91/91 - 0s - loss: 6413044.5082 - mae: 1878.1650 - val_loss: 11007345.0000 - val_mae: 2549.9556\n",
      "Epoch 342/350\n",
      "91/91 - 0s - loss: 6129631.7033 - mae: 1906.8112 - val_loss: 11030145.0000 - val_mae: 2586.2434\n",
      "Epoch 343/350\n",
      "91/91 - 0s - loss: 6339113.6758 - mae: 1893.2766 - val_loss: 11428659.0000 - val_mae: 2782.0967\n",
      "Epoch 344/350\n",
      "91/91 - 0s - loss: 6404925.2143 - mae: 1908.8433 - val_loss: 11096823.0000 - val_mae: 2672.3655\n",
      "Epoch 345/350\n",
      "91/91 - 0s - loss: 6688828.3242 - mae: 2009.6381 - val_loss: 10919043.0000 - val_mae: 2599.4661\n",
      "Epoch 346/350\n",
      "91/91 - 0s - loss: 6627472.2033 - mae: 1941.4277 - val_loss: 11285908.0000 - val_mae: 2805.7510\n",
      "Epoch 347/350\n",
      "91/91 - 0s - loss: 6367386.4011 - mae: 1906.4050 - val_loss: 11058961.0000 - val_mae: 2525.2400\n",
      "Epoch 348/350\n",
      "91/91 - 0s - loss: 6606419.6593 - mae: 1996.3147 - val_loss: 10838851.0000 - val_mae: 2712.2783\n",
      "Epoch 349/350\n",
      "91/91 - 0s - loss: 6361316.5714 - mae: 1899.6445 - val_loss: 10730447.0000 - val_mae: 2637.7312\n",
      "Epoch 350/350\n",
      "91/91 - 0s - loss: 6451860.9121 - mae: 1934.8468 - val_loss: 10816287.0000 - val_mae: 2564.7151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa933257290>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X_train, y_train, epochs=350, batch_size=32, verbose=2, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 10816287.000, RMSE: 3288.812, MAE: 2564.715\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "mse, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('MSE: %.3f, RMSE: %.3f, MAE: %.3f' % (mse, np.sqrt(mse), mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 16709.824\n"
     ]
    }
   ],
   "source": [
    "# make a prediction\n",
    "row = np.asarray([18024.0, 16722.0, 14385.0, 21342.0, 17180.0]).reshape((1, n_steps, 1))\n",
    "yhat = model.predict(row)\n",
    "print('Predicted: %.3f' % (yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
